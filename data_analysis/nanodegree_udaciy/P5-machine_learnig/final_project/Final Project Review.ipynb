{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Identificar fraude no email da Enron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primeira Parte: Fundamentação Teórica e Explicação do Código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo desse projeto é determinar se um funcionário é ou não um funcionário de interesse (POI). Um funcionário de interesse é um funcionário que participou do escândalo da empresa Enron. \n",
    "\n",
    "Para isso iremos usar diversos métodos que foram aprendidos no módulo de *Machine Learning*, mais especificamente usaremos o conjunto de métodos **sklearn**, que contém inúmeras funcionalidades para esse aprendizado :)\n",
    "\n",
    "Meu código final está no arquivo *poi_id.py* que deve ser executado primeiro para exportar os conjuntos de dados e em seguida deve-se executar o *tester.py*.\n",
    "\n",
    "Machine Learning foi a solução ideal para a resolução desse problema pois para sabermos se um funcionário está ou não envolvido no escândalo, nós precisamos que o nosso código saiba o que é isso e como classificar isso.\n",
    "Existem diversos tipos de ML tais como recomendação (que Netflix e Amazon usam e abusam), detecção de anomalias, prevenção de fraude, agrupamento e diversos outros.\n",
    "\n",
    "Para a resolução desse problema eu testei **4 algoritmos** que aprendi ao longo do módulo:\n",
    "- **Decision Tree:** Esse algoritmo pode ser comparado ao *CASE WHEN... THEN.. ELSE... END\". Basicamente nós aplicamos certas regras nos dados e dependendo de cada valor, um decisão é tomada. Esse algoritmo é bastante usado em pesquisas operacionais.\n",
    "- **Random Forest:** É um *ensemble learning* supervisionado, que é comumente usado em problemas de classificação, detecção de anomalias e redes neurais.\n",
    "- **SDG:** Esse método é usado em otimização e tem como objetivo encontrar o mínimo local de uma função.\n",
    "- **NaiveBayes:** É bastante usado em *text learning* \n",
    "\n",
    "Além dos 4 métodos apresentados acima, existem alguns cuidados, ou melhor, tratativas que devem ser aplicadas no dataset antes de treinar o algoritmo.\n",
    "\n",
    "Um ponto bastante importante é a **remoção de outliers**. Classificamos um valor como outlier quando ele está muito discrepante do restante dos valores. Existem algumas técnicas para a detecção/exclusão, uma bastante conhecida e que aprendi aqui no Nanodegree é o [Interquartile Range (IQR)](https://en.wikipedia.org/wiki/Interquartile_range).\n",
    "\n",
    "Nesse projeto tive que remover um valor do conjunto de dados. \n",
    "   - `data_dict.pop('TOTAL')`: *TOTAL* não é uma pessoa e isso estava poluindo a análise.\n",
    "   \n",
    "Algumas métricas iniciais sobre o nosso conjunto de dados:\n",
    "   - **Total de registros:**  145\n",
    "   - **Total de poi:**  18\n",
    "   - **Total de não-poi:**  127\n",
    "   - **Total de atributos:** 23\n",
    "   \n",
    "Outro ponto de limpeza dos dados que precisei fazer foi realizar um replace em todos os registros cujos valores eram **NaN**. Isso poderia afetar bastante as análises, então decidi apenas substituir por **0**:\n",
    "   - `final = df[cols].copy().applymap(lambda x: 0  if x == 'NaN' else x)`\n",
    "   \n",
    "Agora sobre as **features** que foram analizadas aqui, utilizei o algoritmo [SelectPercentile](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html) e [FeatureSelection](http://scikit-learn.org/stable/modules/feature_selection.html) para saber quais features mais eram relevantes para a descoberta da nossa pergunta.\n",
    "O primeiro foi para pegar um percentual do conjunto de dados e o segundo foi utilizado para a seleção das features de fato :)\n",
    "\n",
    "Durante o módulo, nós aprendemos a criar novas features que podem nos ajudar durante a análise... sabendo disso, decidi criar uma funçao chamada *computeFractionPoi*, que retorna o percentual de mensagens que a pessoa enviou para um *POI* e que recebeu de um *POI*.\n",
    "\n",
    "**Por que achei isso útil?**\n",
    "   - Bom, com esses dois valores nós conseguimos saber se a pessoa tinha muito contato com um POI, assim podemos supor que como ela tem muito contato com ele, ela também pode ser um\n",
    "   \n",
    "Infelizmente, essas duas variáveis criadas não tiveram o resultado que eu esperava.\n",
    "Mais pra frente no relatório você vai ver que ela não foi classificada com um das variáveis (features) mais importantes para o nosso modelo :(\n",
    "\n",
    "**Ajustes de escala:**\n",
    "   - Aqui utilizei o método [MinMaxScaler()](http://scikit-learn.org/stable/modules/preprocessing.html) para ajuste nas escalas dos dados. A motivação para usar esta escala inclui robustez a desvios padrões muito pequenos de recursos e preservando zero entradas em dados escassos.\n",
    "\n",
    "Tendo nossas features selecionadas e armazenadas em *features_list*, executei o seguinte código para fazer a divisão em **labels** e **features** para que possamos começar a testar os algoritmos de ML!\n",
    "\n",
    "```\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "```\n",
    "\n",
    "Esse conjunto *featureFormat* foi provido pela Udacity e pode ser encontrado [aqui](https://github.com/udacity/ud120-projects/blob/master/tools/feature_format.py).\n",
    "\n",
    "Por fim, utilizei o método de [cross_validation](http://scikit-learn.org/stable/modules/cross_validation.html) para realizarmos a divisão de nosso dataset em dois, um para teste e outro para treinamento. \n",
    "Realizar essa divisão é essencial, pois ele nos ajuda a evitarmos bastante a ocorrência de *overfitting*, porém alguns cuidados devem ser tomados, como por exemplo, a divisão precisa ser randômica, para que não treinamos nosso algoritmo e deixe ele com um viés mais forte. \n",
    "\n",
    "Para isso, é usado o [StratifiedShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html), que basicamente garante que teremos uma quantidade proporcional de POI vs non-POI. \n",
    "Isso é usado porque pode acontecer de ao usarmos um método comum para split dos dados, pode acontecer de não termos rótulos de POI no conjunto de teste, o que não tornaria o modelo bom o suficiente.\n",
    "\n",
    "Para isso, dentro desse método, existe o [test_train_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) que faz essa divisão e já leva em conta esse cuidado.\n",
    "\n",
    "Aqui eu fiz a divisão clássica de 30% para teste e 70% para treino.\n",
    "\n",
    "Na segunda parte entrarei um pouco mais detalhado no código e em alguns testes e tunings que fiz com alguns parâmetros para que fosse possível obter a melhor resposta :)\n",
    "\n",
    "\n",
    "### Antes de irmos para o código de fato, vou falar um pouquinho sobre o **porque devemos utilizar tuning**.\n",
    "\n",
    "Quando se usa um algoritmo de machine learning, basicamente queremos prever algo utilizando nossos dados, nossas features como fonte da \"inteligência\". Ao usarmos os valores defaults para os parâmetros desses algoritmos, pode ser que eles não seja os ideias para o nosso caso e nos retornem um resultado diferente do que estamos esperando.\n",
    "Ou também quando precisamos de performance dificilmente os valores padrão dos parâmetros nos trarão a performance que queremos.\n",
    "\n",
    "Aqui no relatório, por exemplo, eu fiz testes com o parâmetro do SelectPercentile para que pudesse escolher realmente as melhores features para esse problema. Quando você precisa que seu algoritmo seja usado em sua máxima \"potência\", você precisa entender como ele funciona, para que serve cada parâmetro e, com certeza, alterar esses valores até chegar em um ponto que te deixe satisfeito :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segunda Parte: Código executado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../ud120-projects/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.76691\tPrecision: 0.35142\tRecall: 0.33350\tF1: 0.34223\tF2: 0.33694\n",
      "\tTotal predictions: 11000\tTrue positives:  667\tFalse positives: 1231\tFalse negatives: 1333\tTrue negatives: 7769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## DecisionTreeeClassifier\n",
    "run tester.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.82473\tPrecision: 0.54380\tRecall: 0.22350\tF1: 0.31680\tF2: 0.25334\n",
      "\tTotal predictions: 11000\tTrue positives:  447\tFalse positives:  375\tFalse negatives: 1553\tTrue negatives: 8625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## DecisionTreeeClassifier\n",
    "run tester.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.83255\tPrecision: 0.56280\tRecall: 0.35400\tF1: 0.43462\tF2: 0.38237\n",
      "\tTotal predictions: 11000\tTrue positives:  708\tFalse positives:  550\tFalse negatives: 1292\tTrue negatives: 8450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## NaiveBayesClassifier\n",
    "run tester.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=10, error_score='raise'\n",
      "\testimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1\n",
      "\teta0=0.0, fit_intercept=True, l1_ratio=0.15\n",
      "\tlearning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1\n",
      "\tpenalty='l2', power_t=0.5, random_state=42, shuffle=True, verbose=0\n",
      "\twarm_start=False)\n",
      "\tfit_params={}, iid=True, n_jobs=1\n",
      "\tparam_grid={'loss': ['hinge', 'log', 'squared_hinge'], 'n_iter': [30, 35], 'alpha': [0.01, 0.0001, 1e-06]}\n",
      "\tpre_dispatch='2*n_jobs', refit=True, scoring='f1', verbose=0)\n",
      "\tAccuracy: 0.60445 Precision: 0.16036  Recall: 0.27750 F1: 0.20326 F2: 0.24213\n",
      "\tTotal predictions: 11000  True positives:  555  False positives: 2906 False negatives: 1445 True negatives: 6094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## SDGClassifier with GridSearchDV\n",
    "run tester.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode-se perceber, aqui não precisei tunar o modelo para conseguir o mínimo requerido para o projeto (0.3 para ambas as métricas).\n",
    "\n",
    "Quando calculamos a métrica de precisão, queremos saber qual é a razão entre os eventos que previmos corretamente com todos os outros que eram corretos, ou seja, se em nosso conjunto previmos que temos 100 POIs, mas na verdade apenas 30 realmente são, significa que tivemos uma precisão de 30%.\n",
    "Já recall é quando queremos saber a taxa de verdadeiros positivos no problema, ou seja, se no conjunto todo temos 300 POIs, significa que tivemos 33% de recall (100/300)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um tratamento que fiz foi selecionar o melhor *percentile* para o conjunto de dados. O que esse método faz é selecionar as **features de acordo com o seu score**.\n",
    "\n",
    "Seguem abaixo alguns testes que fiz até chegar no melhor modelo :)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### SelectPercentile: 10\n",
    "GaussianNB(priors=None)\n",
    "\tAccuracy: 0.82182\tPrecision: 0.51818\tRecall: 0.28500\tF1: 0.36774\tF2: 0.31319\n",
    "\tTotal predictions: 11000\tTrue positives:  570\tFalse positives:  530\tFalse negatives: 1430\tTrue negatives: 8470\n",
    "\n",
    "#### SelectPercentile: 20\n",
    "GaussianNB(priors=None)\n",
    "\tAccuracy: 0.82391\tPrecision: 0.52704\tRecall: 0.30700\tF1: 0.38799\tF2: 0.33497\n",
    "\tTotal predictions: 11000\tTrue positives:  614\tFalse positives:  551\tFalse negatives: 1386\tTrue negatives: 8449\n",
    "\n",
    "#### SelectPercentile: 25 => Best Option!!! :-)\n",
    "GaussianNB(priors=None)\n",
    "\tAccuracy: 0.83255\tPrecision: 0.56280\tRecall: 0.35400\tF1: 0.43462\tF2: 0.38237\n",
    "\tTotal predictions: 11000\tTrue positives:  708\tFalse positives:  550\tFalse negatives: 1292\tTrue negatives: 8450\n",
    "\n",
    "#### SelectPercentile: 30\n",
    "GaussianNB(priors=None)\n",
    "\tAccuracy: 0.82782\tPrecision: 0.53991\tRecall: 0.35850\tF1: 0.43089\tF2: 0.38433\n",
    "\tTotal predictions: 11000\tTrue positives:  717\tFalse positives:  611\tFalse negatives: 1283\tTrue negatives: 8389\n",
    "\n",
    "#### SelectPercentile: 40\n",
    "GaussianNB(priors=None)\n",
    "\tAccuracy: 0.81817\tPrecision: 0.43627\tRecall: 0.31150\tF1: 0.36348\tF2: 0.33040\n",
    "\tTotal predictions: 12000\tTrue positives:  623\tFalse positives:  805\tFalse negatives: 1377\tTrue negatives: 9195\n",
    "\n",
    "#### SelectPercentile: 50\n",
    "GaussianNB(priors=None)\n",
    "\tAccuracy: 0.75583\tPrecision: 0.27449\tRecall: 0.28300\tF1: 0.27868\tF2: 0.28126\n",
    "\tTotal predictions: 12000\tTrue positives:  566\tFalse positives: 1496\tFalse negatives: 1434\tTrue negatives: 8504"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eu usei o NaiveBayes como teste para achar o melhor Percentile :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meu conjunto de features final ficou sendo o seguinte:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "['poi', 'deferred_income', 'bonus', 'total_stock_value', 'salary', 'exercised_stock_options']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Terceira Parte: Possíveis problemas com o conjunto de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui irei pontuar em tópicos alguns problemas que encontrei nesse conjunto de dados:\n",
    "   - Inconsistência nos e-mails\n",
    "   - Diferentes conjuntos de dados podem introduzir diferentes bias e erros\n",
    "   - POI pode não estar no conjunto de dados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
